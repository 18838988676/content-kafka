spring.application.name=kafka
server.port=8080
#============== kafka ===================
# 指定kafka 代理地址，可以多个,192.168.1.120 window10 IP地址的；  主题  win10->docker ->centos-> vim /etc/hosts -> 172.*。*。*。   容器名字； 需要改成127.0.0.1
#即：在zk上的地址 需是localhost  或者127.0.0.1  而不是容器名称！！！！ 而不是容器名称！！！！ 而不是容器名称！！！！ 而不是容器名称！！！！==》  vim /etc/hosts
spring.kafka.bootstrap-servers=192.168.1.108:9092,192.168.1.108:9093,192.168.1.108:9094

#=============== provider  =======================
spring.kafka.producer.retries=0
# 每次批量发送消息的数量
spring.kafka.producer.batch-size=16384000
spring.kafka.producer.buffer-memory=33554432000
spring.kafka.producer.acks=-1
# 指定消息key和消息体的编解码方式
spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer

#=============== consumer  =======================
# 指定默认消费者group id
#spring.kafka.consumer.group-id=ordertopic1

spring.kafka.consumer.auto-offset-reset=earliest
spring.kafka.consumer.enable-auto-commit=true
spring.kafka.consumer.auto-commit-interval=100

# 指定消息key和消息体的编解码方式
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer


#logging.level.com.cn.kafka01.produce=info
##logging.path=
## 不指定路径在当前项目下生成springboot.log日志
## 可以指定完整的路径；
##logging.file=G:/springboot.log
## 在当前磁盘的根路径下创建spring文件夹和里面的log文件夹；使用 spring.log 作为默认文件
#logging.path=/spring/log
## 在控制台输出的日志的格式
#logging.pattern.console=%d{yyyy-MM-dd}[%thread]%-5level %logger{50}-%msg%n
## 指定文件中日志输出的格式
##logging.pattern.file=%d{yyyy-MM-dd}===[%thread]===%-5level===%logger{50}==== %msg%n
#logging.pattern.file=%msg%n
logging.config=classpath:logback-spring.xml
logging.level.root=INFO
CY_LOG_HOME=/install/f_k_s_h_job/f_k_s_h_job_logs